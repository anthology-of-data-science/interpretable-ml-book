# Leave One Feature Out (LOFO) Importance {#lofo}

{{< include _setup.qmd >}}

Leave One Feature Out (LOFO) Importance, measures a feature's importance by retraining the model without the feature and comparing the predictive performance.[^loco]

The intuition behind LOFO Importance: If dropping a feature makes the predictive performance worse, then it was an important features.
If dropping the feature keeps performance unchanged it was not important.
LOFO importance may even be negative when removing the feature *improves* the model performance.
To get the LOCO importance for all of the $p$ features, we have to retrain the model p times, each time with a different feature removed from the training data, and measure the resulting model's performance on test data.

This makes LOFO a quite simple algorithm.
Let's still make it little more official by formalizing it in an algorithm:

**Input:** Trained model $\hat{f}$, training features $\mathbf{X}_{\text{train}}$, testing features $\mathbf{X}_{\text{test}}$, training targets $\mathbf{y}_{\text{train}}$, testing targets $\mathbf{y}_{\text{test}}$, and error measure $L$.  
**Procedure:**  

1. **Measure the original model error:**  
   $$
   e_{\text{orig}} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} L\big(y_{\text{test}}^{(i)}, \hat{f}(\mathbf{x}_{\text{test}}^{(i)})\big)
   $$  

2. **For each feature $j \in \{1, \ldots, p\}$:**
   - Remove feature $j$ from the dataset, creating new datasets $\mathbf{X}_{\text{train},-j}$ and $\mathbf{X}_{\text{test},-j}$, which exclude feature $j$.  
   - Train a new model $\hat{f}_{-j}$ on $(\mathbf{X}_{\text{train},-j}, \mathbf{y}_{\text{train}})$.  
   - Measure the new error on the modified test set:  
     $$
     e_{-j} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} L\big(y_{\text{test}}^{(i)}, \hat{f}_{-j}(\mathbf{x}_{\text{test},-j}^{(i)})\big)
     $$  

3. **Calculate LOFO importance:**  
   - As a quotient:  
     $$
     LOFO_j = \frac{e_{-j}}{e_{\text{orig}}}
     $$  
   - Or as a difference:  
     $$
     LOFO_j = e_{-j} - e_{\text{orig}}
     $$  

4. **Sort and visualize** the features by descending importance $LOFO_j$.


To train and retrain the model use the training data, and for measuring the performance you use the test data.
This sounds like common sense, but for other interpretation methods that don't rely on performance measurements (like the PDP) it's okay to use training data (at least I haven't found a general strong reaason not to).



## Examples 

Let's try LOFO for a random forest predicting penguins species from body measurements.
For this I have trained a random forest on 2/3 of the data, leaving 1/3 of the data for error estimation. 

```{r}
#| label: fig-lofo-penguins
#| fig-cap: "LOFO importances for the penguin data."
test_prediction <- predict(penguins_rf, newdata = penguins_test)
test_accuracy <- mean(penguins_test$species == test_prediction)

# Compute LOFO feature importance
feature_importances <- data.frame(feature = setdiff(names(penguins2), 'species'), importance = 0)

for (feature in feature_importances$feature) {
  new_train_data <- penguins_train %>% select(-!!sym(feature))
  new_rf_model <- randomForest(species ~ ., data = new_train_data)
  new_test_prediction <- predict(new_rf_model, newdata = penguins_test %>% select(-!!sym(feature)))
  new_test_accuracy <- mean(penguins_test$species == new_test_prediction)
  feature_importances$importance[feature_importances$feature == feature] <- test_accuracy - new_test_accuracy
}

# Order the features by importance
feature_importances <- feature_importances %>%
  arrange(desc(importance))

# Visualize the feature importances
ggplot(feature_importances, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Feature", y = "LOFO Importance (accuracy difference)") +
  my_theme()

```

@fig-lofo-penguins shows that the bill length was the most important feature according to LOFO, leading to an error increase of over 15 percentage points in accuracy.
Note that because of using accuracy here, where larger is better, I multiplied the importance by minus one.
LOFO also tells us that we could safely remove each of the feature sex, flipper_length_mm, and body_mass_g without affecting model performance.
Note that this insight is only about removing each figure individually, not all together.


For the next example, we predict bike rentals based on weather and calendar information using a random forest trained on 2/3 of the data.

```{r}
#| label: fig-lofo-bike
#| fig-cap: "LOFO feature importances for the bike rental data."
# Split the data into training and testing sets
test_prediction <- predict(bike_rf, newdata = bike_test)
test_mae <- mean(abs(bike_test$cnt - test_prediction))

# Compute LOFO feature importance
feature_importances <- data.frame(feature = setdiff(names(bike_train), 'cnt'), importance = 0)

for (feature in feature_importances$feature) {
  new_train_data <- bike_train %>% select(-!!sym(feature))
  new_mod = randomForest(cnt ~ ., data = new_train_data)
  new_test_prediction <- predict(new_mod, newdata = bike_test %>% select(-!!sym(feature)))
  new_test_mae <- mean(abs(bike_test$cnt - new_test_prediction))
  feature_importances$importance[feature_importances$feature == feature] <- new_test_mae - test_mae 
}

# Order the features by importance
feature_importances <- feature_importances %>%
  arrange(desc(importance))

# Visualize the feature importances
ggplot(feature_importances, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Feature", y = "LOFO Importance (MAE difference)") +
  my_theme()

```

@fig-lofo-bike shows that temperature and humidity are the most important features accoriding to LOFO.
Interestingly the month has a negative importance, which has implications for feature selection.
Because how LOFO works algorithmically, we now know that removing the feature increases the model performance.


Let's try something.
To simulate an extreme version of correlated features, I added another copy of temperature to the dataset.
Then I trained again the "original" model on the duplicated temperature data.

```{r}
#| label: fig-lofo-bike-correlated
#| fig-cap: "LOFO feature importances for the bike rental data."
# Split the data into training and testing sets
bike2 = bike
bike2$temp_copy = bike$temp
bike2_train <- bike2[bike_train_index, ]
bike2_test <- bike2[-bike_train_index, ]
bike2_rf = randomForest(cnt~ ., data = bike2_train)


test_prediction <- predict(bike2_rf, newdata = bike2_test)
test_mae <- mean(abs(bike2_test$cnt - test_prediction))

# Compute LOFO feature importance
feature_importances <- data.frame(feature = setdiff(names(bike2_train), 'cnt'), importance = 0)

for (feature in feature_importances$feature) {
  new_train_data <- bike2_train %>% select(-!!sym(feature))
  new_mod = randomForest(cnt ~ ., data = new_train_data)
  new_test_prediction <- predict(new_mod, newdata = bike2_test %>% select(-!!sym(feature)))
  new_test_mae <- mean(abs(bike2_test$cnt - new_test_prediction))
  feature_importances$importance[feature_importances$feature == feature] <- new_test_mae - test_mae 
}

# Order the features by importance
feature_importances <- feature_importances %>%
  arrange(desc(importance))

# Visualize the feature importances
ggplot(feature_importances, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Feature", y = "LOFO Importance (MAE difference)") +
  my_theme()
```

@fig-lofo-bike-correlated shows an interesting phenomenon: 
The feature importances of both temperature features is slightly negative.
Now we could wrongly conclude that temperature isn't important.
If we would want to be double wrong we might conclude that removing both temperature features would be good.
But we know for a fact from our earlier analysis [@fig-lofo-bike] that removing temperature leads to a steep performance drop.
Two learnings from that example:

- LOFO Importance of a strongly correlated feature is automatically low, since LOFO importance is to be interpreted conditionally on the information provided by the other features. And if you already have a temperature feature in the data, the perfect copy of that same feature is not an important additional information.
- When you use LOFO Importance as information for feature selection, beware of the interpretation: LOFO only indicates how the model performance reacts to *individually* removing features. As @fig-lofo-bike-correlated showed, LOFO doesn't give us the information how the performance changes when removing 2 or more features at once. 


## How LOFO relates to other methods 

LOFO differs from the other methods presented in this book, since most of the other methods in this book don't require to retrain the model.
Arguably, LOFO still follows a classic post-hoc, model-agnostic method patterns, since it can be applied to any model and retraining the model doesn't affect the original model that we want to interpret.
However, due to retraining the model, the interpretation shifts from only interpreting that one single model, and more to the learner and how model training reacts to changes in the features.

To me the biggest question is: How does LOFO compare to [PFI](#pfi) and when should you use which importance?
Let's start with the similarities:
Both PFI and LOFO are performance-based importances measures; both compute importance by "removing" the information of a feature, even when the do it in different; both work in a one-at-a-time way, at least in their simplest version. 
As we already saw in the example @fig-lofo-bike-correlated, LOFO has a conditional interpretation of importance.
It's only about a feature's additional predictive value it brings to the table with the other model features sitting that that same table.
This already distinguishes it from *marginal* feature importance, which also has a marginal interpretation.
However, there is also a conditional version of PFI, and LOFO is closer to this, with similar interpretations.
But since conditional PFI and LOFO work differently, they still are different in their interpretations.
Removing a feature and retraining a model allows the algorithm to create a new model and potentially offset the missing feature with other features.
That means that LOFO's interpretation is more for the learner, the ML algorithm, whereas conditional PFI sticks to the one model and doesn't involve other models produced by the ML algorithm.

LOFO is just much more related to feature selection methods.

Note that again the notion of "importance" is defined through what the method does.
And in the case of LOFO importance, how we may interpret the results is based on the fact that we retrain the model.

CONTINUE HERE

## Advantages

**Implementing LOFO is simple**.
For the examples here I did just that.
You can do it yourself, you don't actually need any package for that.

LOFO bridges interpretability and features selection: While permutation features importance and other post-hoc methods that don't retrain the model seem to give valuable insights also in terms of feature selection, LOFO actually does.
A feature with low LOFO importance can be safely removed from the model, while you can't make that same move with PFI. 

LOFO assigns negative importance to features that hurt performance.

LOFO avoids the correlation problem:
By removing the feature alltogether instead of permuting it, we don't generate new data points that might extrapolate.

LOFO makes sense when your goal of interpretation is about the phenomenon (and less about the model itself). 

You can also gropu features.


## Disadvantages

If you compute LOFO importance for all features, you have to retrain the model $p$ times.
This can be costly, especially when compared to Permutation Feature Importance.


Once you remove a feature and retrain the model, it's unclear what this model exactly represents.
Because the new model that we use to 


TODO: think about implications for causal models. e.g. confounder, mediator, effect, cause

A bit unclear whether to just train the same model with exact same hyperparameters, or run hyperparameter optim again.
For the examples I used the exact same settings.
If you would also do hyperparameter optimization again, you will have a different interpretation.
So insight really is more into the learner than a specific model.

The model

If you have two highly correlated features, they will both get a low LOFO importance.
It's to be expected from the method, but can be highly misleading for LOFO users who don't no this.
You always have to know the dependences between features before interpreting LOFO. 
A "fix" to this would be to group highly correlated features and only compute a shared importance for them.

## Software and Alternatives

cONTINUE HERE

- Python implementation: https://github.com/aerdem4/lofo-importance

- Knockoffs are indistunguishable copies of the variables, at least basedon distribution, but which are independent of X.

- In general overlap with feature selection techniques.

[^loco]: The first formal description of LOFO I'm aware of is by @lei2018distributionfree. They call it Leave One Covariate Out, and the paper is mostly about distribution-free testing. However due to the approaches simplicity and use in feature selection, it doesn't make sense to attribute it to a single publication.


