# Methods Overview

<!-- 

TODOs:

- Link each to the goals of interpretability
0 

-->


This chapter will get more into the weeds of what options we have to make machine learning more interpretable, while the previous chapter was a more general primer on interpretability.
The goal of this chapter is to provide a map of approaches so that when diving into the individual methods, you still realize you are in a forest when looking at an individual trees. 

@fig-taxonomy provides a taxonomy of the different approaches.

```{mermaid}
%%| label: fig-taxonomy
%%| fig-cap: Short taxonomy of IML methods which also reflects the structure of the book.
%%| fig-align: center
graph TD
    A[ML interpretability]
    A --> B[by-design]
    A --> C[post-hoc]
    C --> D[model-agnostic]
    C --> E[model-specific]
    D --> F[local]
    D --> G[global]
```

In general, we can distinguish between interpretability by design and post hoc interpretability.
**Interpretability by design** means that we train inherently interpretable models, like using logistic regression instead of a random forest.
**Post-hoc interpretability** means that we use an interpretability method after the model has been trained.
Post-hoc interpretation methods can be **model-agnostic**, like permutation feature importance or **model-specific**, like analyzing the features learned by a neural network.
Model-agnostic methods can be further divided into **local** methods which focus on individual predictions, and **global** methods which focus on datasets.
This book focuses on post-hoc model-agnostic methods, but also covers basic models that are interpretable by design and model-specific methods for neural networks.

Let's look into each category of interpretability, understand what makes them different, learn about theirs strengths and weaknesses, and ultimately see that it's not an either-or decision.
Strengths and weaknesses strongly depends on the [goal of interpretation](#goals).

## Interpretable models by design 

Interpretability by design is decided on the level of the machine learning *algorithm*.
If you want an ML algorithm that produces interpretable models, the algorithm has to constrain the search of models to those that are interpretable.
The simplest example is linear regression:
When you use ordinary least squares to fit/train a linear regression model, you are using an algorithm that will only find models that are linear in the input features.
Models that are interpretable by design are also called **intrinsically** or **inherently** interpretable models.

![](./images/interpretable-box.png)

This book covers the most basic interpretability by design approaches:

- [Linear regression](#limo)
- [Logistic regression](#logistic)
- [Linear model extensions](#extend-lm)
- [Decision trees](#tree)
- [Decision rules](#rules)
- [RuleFit](#rulefit)


<!-- more than completely interpretable models -->
But these are only the most basic ones.
Since interpretability is such a fluffy concept, the section of interpretable models is huge and the models can be very different.
Just a few examples:


- Prototype-based neural network for image classification [@ma2024interpretable]. Here the interpretable part is that a classification is a weighted sum of prototype images with highlighted sections of the image. An example of case-based reasoning.
- @yang2024inherently and suggest "inherently interpretable tree ensemble" and one part is to adjust hyperparameters (like maximum tree depth of only 2 or 3) and adding monotonicity constraint among others to make them more interpretable by design. The paper also introduces other pruning steps to reduce number of effects. 
- TODO: more examples here


All interpretability-by-design approaches have in common that they affect how the final model looks like.
It relies to using specific constraints in the training process so that at the end of the process "interpretable" models come out.
What is then interpretable is very broad and very different across proposed ML algorithms.

<!-- Strengths -->

CONTINUE HERE

Strong on these goals: Insight into the model, justification as long as there is a match between interpretability of the model towards target audience.
Justification is very strong, especially, by definition, for the established things like, logistic regression in medical research.
But since the models are so diverse and their interpretability mode as well, can't give definit answer on how well they match the goals, just some general thoughts.

Often well-established, like logistic regression in clinical research.

However, interpretability-by-design is itself bigger than just intepretability.
Since when you can design for interpretability, you typically also design for other things, like robustness and infusion of domain knowledge.


<!-- Limitations -->
Not so strong on these goals:
Insight into the data is only given when the model structure and assumptions reflect reality.
Is your true outcome in reality a weighted sum of the inputs?



Not agreed upon if its really interpretable.

Has a cost: restricting to interrpetable models may exclude models that have way better predictive performance.
For example, winners of Kaggle are typically xgboost for tabular dta and neural networks for image.
Adding constraints has the potential of making the model less performant.
In terms of a search, it makes the search space smaller.
Let's say that the optimal model would be a support vector machine, but if you only allow linear regression mdoels, you will have a sub-optimal model.
So there is typically a trade-off between interpretability by design and predictive performance.


Conceptual problem: When it comes to discovery, we kind of rely on the the model being a good representation of reality.
Difficutl to argue when there are more predictive models.
And often relies on assumptions
When used for answering questions, they rely on assumptions that the model reflects reality ("Is the ")

Rashomon effect:


## Post-Hoc Interpretability

Post-hoc methods are all methods that are applied after the model was trained.
This means they don't affect the model training.

::: {.callout-tip}

You can apply post-hoc methods to models that are interprerable by-design.

:::

How does this work?
There's two routes to this:

- Model-agnostic: We ignore what's *inside* the model and only analyze howthe prediction function behaves. 
- Model-specific: We take some parts of the model and try to analyze them to better understand the model. This can work by assigning meaning to parts of the model, like trying to see what a neuron in a neural network responds most to, but also give us feature-based summaries like feature importance (e.g. Gini importance in random forests).



### Model-Agnostic Methods {#agnostic}

Separating the explanations from the machine learning model (= model-agnostic interpretation methods) has some advantages (Ribeiro, Singh, and Guestrin 2016 [@ribeiro2016model]).
The great advantage of model-agnostic interpretation methods over model-specific ones is their flexibility.
Machine learning developers are free to use any machine learning model they like when the interpretation methods can be applied to any model.
Anything that builds on an interpretation of a machine learning model, such as a graphic or user interface, also becomes independent of the underlying machine learning model.
Typically, not just one, but many types of machine learning models are evaluated to solve a task, and when comparing models in terms of interpretability, it is easier to work with model-agnostic explanations, because the same method can be used for any type of model.

![](./images/agnostic-black-box.png)

Desirable aspects of a model-agnostic explanation system are (Ribeiro, Singh, and Guestrin 2016):

- **Model flexibility:**
The interpretation method can work with any machine learning model, such as random forests and deep neural networks.
- **Explanation flexibility:**
You are not limited to a certain form of explanation.
In some cases it might be useful to have a linear formula, in other cases a graphic with feature importances.
- **Representation flexibility:**
The explanation system should be able to use a different feature representation as the model being explained.
For a text classifier that uses abstract word embedding vectors, it might be preferable to use the presence of individual words for the explanation.

However, some argue we shouldn't try to explain black-box algorithms at all, at least not when it comes to high-stakes decisions. [@rudin2019stop]


**The bigger picture**

Let's take a high level look at model-agnostic interpretability.
We capture the world by collecting data, and abstract it further by learning to predict the data (for the task) with a machine learning model.
Interpretability is just another layer on top that helps humans understand.


![The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in the form of explanations.](./images/big-picture.png){#fig-big-picture}

The lowest layer is the **World**.
This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also more abstract things like the real estate market.
The World layer contains everything that can be observed and is of interest.
Ultimately, we want to learn something about the World and interact with it.

The second layer is the **Data** layer.
We have to digitize the World in order to make it processable for computers and also to store information.
The Data layer contains anything from images, texts, tabular data and so on.

By fitting machine learning models based on the Data layer, we get the **Black Box Model** layer.
Machine learning algorithms learn with data from the real world to make predictions or find structures.

Above the Black Box Model layer is the **Interpretability Methods** layer, which helps us deal with the opacity of machine learning models.
What were the most important features for a particular diagnosis?
Why was a financial transaction classified as fraud?

The last layer is occupied by a **Human**.
Look! This one waves to you because you are reading this book and helping to provide better explanations for black box models!
Humans are ultimately the consumers of the explanations.

This multi-layered abstraction also helps to understand the differences in approaches between statisticians and machine learning practitioners.
Statisticians deal with the Data layer, such as planning clinical trials or designing surveys.
They skip the Black Box Model layer and go right to the Interpretability Methods layer.
Machine learning specialists also deal with the Data layer, such as collecting labeled samples of skin cancer images or crawling Wikipedia.
Then they train a black box machine learning model.
The Interpretability Methods layer is skipped and humans directly deal with the black box model predictions.
It's great that interpretable machine learning fuses the work of statisticians and machine learning specialists.

Of course this graphic does not capture everything:
Data could come from simulations.
Black box models also output predictions that might not even reach humans, but only supply other machines, and so on.
But overall it is a useful abstraction to understand how interpretability becomes this new layer on top of machine learning models.

Model-agnostic interpretation methods can be further distinguished into local and global methods.
The book is also organized according to this distinction.
[Global methods](#global-methods) describe how features affect the prediction **on average**.
In contrast, [local methods](#local-methods) aim to explain **individual predictions**.


#### Global Model-Agnostic Methods {#global-methods}

Global methods describe the average behavior of a machine learning model.
The counterpart to global methods are [local methods](#local-methods).
Global methods are often expressed as expected values based on the distribution of the data.
For example, the [partial dependence plot](#pdp), a feature effect plot, is the expected prediction when all other features are marginalized out.
Since global interpretation methods describe average behavior, they are particularly useful when the modeler wants to understand the general mechanisms in the data or debug a model.

In this book, you will learn about the following model-agnostic global interpretation techniques:

* The [partial dependence plot](#pdp) is a feature effect method.
* [Accumulated local effect plots](#ale) is another feature effect method that works when features are dependent.
* [Feature interaction (H-statistic)](#interaction) quantifies to what extent the prediction is the result of joint effects of the features.
* [Functional decomposition](#decomposition) is a central idea of interpretability and a technique that decomposes the complex prediction function into smaller parts.
* [Permutation feature importance](#feature-importance) measures the importance of a feature as an increase in loss when the feature is permuted.
* [Global surrogate models](#global) replaces the original model with a simpler model for interpretation.
* [Prototypes and criticisms](#proto) are representative data point of a distribution and can be used to enhance interpretability.



<!-- strengths and limitations -->
For the goal of model improvements, global methods are king.
For justification of the model, they may also be very useful, but of course less so when it comes to justifying individual predictions, that's what local methods are for.


::: {.callout-tip}

Model-agnostic interpretation methods are for entire datasets.
But you are typically free to modify that dataset before e.g. computing feature importance or plotting an effect.
By subsetting your data, you can turn these "global" methods into "group-wise" methods, which also bridge the divide between global and local methods.
We will see this in action in the examples in this book.

:::


#### Local Model-Agnostic Methods {#local-methods}

Local interpretation methods explain individual predictions.
In this chapter, you will learn about the following local explanation methods:

* [Individual conditional expectation curves](#ice) are the building blocks for [partial dependence plots](#pdp) and describe how changing a feature changes the prediction.
* [Local surrogate models (LIME)](#lime) explain a prediction by replacing the complex model with a locally interpretable surrogate model.
* [Scoped rules (anchors)](#anchors) are rules that describe which feature values anchor a prediction, in the sense that they lock the prediction in place.
* [Counterfactual explanations](#counterfactual) explain a prediction by examining which features would need to be changed to achieve a desired prediction.
* [Shapley values](#shapley) are an attribution method that fairly assigns the prediction to individual features.
* [SHAP](#shap) is another computation method for Shapley values, but also proposes global interpretation methods based on combinations of Shapley values across the data.

LIME and Shapley values are attribution methods, so that the prediction of a single instance is described as the sum of feature effects.
Other methods, such as [counterfactual explanations](#counterfactual), are example-based.

<!-- Strengths and Limitations -->
When it comes to debugging the model, local methods are not the first pick.
But they can be helpful in identifying specific ways in which your model has problems, especially if they are only for certain subsets of the data.
You can also randomly sample data points, explain their predictions, and get an idea of how to improve the model.


### Model-specific methods

As the name says, these are 

![](./images/specific-black-box.png)

Like Gini importance etc.

<!-- Why not interpretable -->
To make predictions with a neural network, the data input is passed through many layers of multiplication with the learned weights and through non-linear transformations.
A single prediction can involve millions of mathematical operations depending on the architecture of the neural network.
There is no chance that we humans can follow the exact mapping from data input to prediction.
We would have to consider millions of weights that interact in a complex way to understand a prediction by a neural network.
To interpret the behavior and predictions of neural networks, we need specific interpretation methods.
The chapters assume that you are  familiar with deep learning, including convolutional neural networks.

<!-- Why specific interpretation -->
We can certainly use [model-agnostic methods](#agnostic), such as [local models](#lime) or [partial dependence plots](#pdp), but there are two reasons why it makes sense to consider interpretation methods developed specifically for neural networks:
First, neural networks learn features and concepts in their hidden layers and we need special tools to uncover them.
Second, the gradient can be utilized to implement interpretation methods that are more computationally efficient than model-agnostic methods that look at the model "from the outside".
Also most other methods in this book are intended for the interpretation of models for tabular data.
Image and text data require different methods.

<!-- Strengths and Limitations -->
The biggest strength of these methods is to discover insights about the models themselves.
This may also help for improving the model and justifying it towards other people.
Model-specific methods can be helpful for debugging and improving the models, but it's difficult to make general statements as these may do so



The neural network chapters cover the following techniques that answer different questions:

- [Learned Features](#feature-visualization): What features has the neural network learned?
- [Pixel Attribution (Saliency Maps)](#pixel-attribution): How did each pixel contribute to a particular prediction?
- [Concepts](#neural-concepts): Which more abstract concepts has the neural network learned?
-  [Adversarial Examples](#adversarial) are closely related to [counterfactual explanations](#counterfactual): How can we trick the neural network?
-  [Influential Instances](#influential) is a more general approach with a fast implementation for gradient-based methods such as neural networks: How influential was a training data point for a certain prediction?





## Combine them 

Don't think you have decide for one approach.
Be flexible.
Examples:

- You train random forest, but find out that logistic regression works with similar performance, maybe slighlty less. You switch to logistic regressio nand reap the benefits of interpreting the coefficients.
- Due to regulatory reasons, you are only allowed to use decision-rules. However, for better debugging, you still rely on model-agnostic methods like [LOFO](#lofo).
- You train an tree ensemble with xgboost. You find out that setting max depth to two produces good enough models and comes with the advantage of 

The lines are blurry anyways:

- Many wouldn't hesitate to put "logistic regression" as interpretability-by-design. But to get the log odds, we have to do post-hoc interpretation, in the sense that we have to compute the log odds. It's just a transfomration of the coefficients, but where is the transiation between that and e.g. neuraon activation maximization?  
- [SHAP](#shap) is a "model-agnostic" method, but when applied to a linear regression model, it gives you pre-defined effect values that are centered by the expected effects. If you had calculated the centered effects, then you would call it still intepretable by-design.
- There are also approach that make models more interpretable, but only if you apply some post-hoc processes to them, like tree ensembles with post-hoc.
- By-design, you can enforce xgboost to only boost monotonic relationships. You wouldn't call this approach fully interpretable, but it still is interpretability by design, since now you can do something like giving a sign for each feature saying whether increases in teh feature are related to rising or lowering predictions.



## Attic

I have a pet theory that we can categorize "inherently interpretable mdoels" into 3 categories.
These are true for both the repreentation of models and how we produce explanations.

- Weighted sum of features: Allows interpretation of the coefficients as linear effects. Example: linear regression.
- Decision rules: Makes for simple if-else explanations. Example: Decision tree.
- Reference data points: Example: kmeans.

Examples for weighted sums:

- WE have the linear regression model
- But also the logistic model is a weighted sum, just on another level
- same goes for all the lm extensions


The weighted sums allow us to assign a value to a feature (coefficient or weight)

For if-else rules we have decision rules and decision lists.

Then there are also combinations between the two, like rulefit 

Not represented here are the other models with data points.
like k-means or k-nn.
These are typically not cited as interpretable, since it only works when you can easily understand the other data points. 






