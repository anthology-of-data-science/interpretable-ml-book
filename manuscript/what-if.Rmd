```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

\newpage

## What-If Plots {#what-if}

What-If plots are the simplest analysis tool there is:
How does the prediction of a data point change when we change one feature?
They are also known under the name ceterus paribus analysis, which is latin for the other things equal.

It's almost to simple to include in this book, or is it?
Well, I figured it's the best way to get started as it is very intuitive, and bridges to many other methods, like ICE (LINK) or counterfactual explanations (LINK).
And it can work well to fix problems with attribution-based techniques, like LIME and SHAP (LINK BOTH)

Here is the algorithm:

- Pick a data point to explain
- Pick a feature
- Pick different values for the feature
- Plot all the values as a line (for continuous feature) or dots and highlight the current data points' value

For step 3 we can now of course try to find different scenarios, meaning certain values.
But from a information and visualization stand point, it really makes more sense to use the entire range if you have a continuous feature, and all categories for categorical features.
Unless, of course, there's just too many categories.

What I love about this: it highlights how most of the model-agnostic methods work:
We get information out of the model by manipulating the input data and observing how the output changes.
Many methods get quite sophisticated about it, like Shapley values which bring in huge background of game theory and algorithms etc.. 
Of course what-if analysis is quite limited.


Here are the connections to other methods:

- The what-if analysis is basically the ICE curve (LINK) for just one data point, at least when we plot the entire range. And the ICE can then be extended to the PDP by averaging the lines. 
- Also ALE plots are aggregations of what-if analysis
- 

And there are cases where the what-if analysis, especially when looking at local changes around the data point can really compliment other interpretations:
Some interpretation methods for explaining individual predictions assing values to the features and a pitfall is to misunderstand them as like coefficients.
This is true for Shapley Values / SHAP, and also for Anchors (??), to a slight degree for LIME (especially when looking at the neighborhood issue).
In fact, a combination of what-if analysis and shapley values won me a prize in an explainability competition.

### Examples



### Advantages

- Super simple
- One feature at a time
- Good for education purposes
- Can be used to fix limitations of attribution methods. Attribution methods, especially Shapley values and ShAp (LINK BOTH) have the problem that 
- As with many methods, it's possible to change the feature so much that it has values that don't mix well with the rest, however, it's much less severe as we have just one data point and, for some applications, it might be easier to understand whehter it's extrapolation or not.

### Disadvantages

- Only one feature at a time
- Therefore no analysis of interactions or anything
- You have no notion of feature importance, e.g. you have to select the feature and don't get a feel of whether this particular what-if feature is really crucial to look at. But you can combine it with other interpretation methods to get a feel for it

### Software and Alternatives

- The [ceterisParibus R package](https://cran.r-project.org/web/packages/ceterisParibus/index.html) implements Ceteris Paribus Profiles, which are What-If Plots for the entire feature range (as I also recommended to do in this chapter, which was actually inspired by this package)  
- Google offers a [What-If Tool](https://pair-code.github.io/what-if-tool/), however this is about.
