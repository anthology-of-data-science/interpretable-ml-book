## Individual Conditional Expectation (ICE) {#ice}

{{< include _setup.qmd >}}

Individual Conditional Expectation (ICE) plots display one line per instance that shows how the instance's prediction changes when a feature changes. 

The partial dependence plot for the average effect of a feature is a global method because it does not focus on specific instances, but on an overall average.
The equivalent to a PDP for individual data instances is called individual conditional expectation (ICE) plot (Goldstein et al. 2017[^Goldstein2017]).
An ICE plot visualizes the dependence of the prediction on a feature for *each* instance separately, resulting in one line per instance, compared to one line overall in partial dependence plots.
A PDP is the average of the lines of an ICE plot.
The values for a line (and one instance) can be computed by keeping all other features the same, creating variants of this instance by replacing the feature's value with values from a grid and making predictions with the black box model for these newly created instances.
The result is a set of points for an instance with the feature value from the grid and the respective predictions.

What is the point of looking at individual expectations instead of partial dependencies?
Partial dependence plots can obscure a heterogeneous relationship created by interactions.
PDPs can show you what the average relationship between a feature and the prediction looks like.
This only works well if the interactions between the features for which the PDP is calculated and the other features are weak.
In case of interactions, the ICE plot will provide much more insight.

A more formal definition:
In ICE plots, for each instance in $\{(x_{S}^{(i)},x_{C}^{(i)})\}_{i=1}^N$ the curve $\hat{f}_S^{(i)}$ is plotted against $x^{(i)}_{S}$, while $x^{(i)}_{C}$ remains  fixed.

### Examples

Let's go back to the [penguin classification task](#penguins) and see how the prediction of each instance is related to the feature "bill_length_mm".
We will analyze a random forest that predicts the probability of different species given body measurements.
In the [partial dependence plot](#pdp) we saw that a larger bill length leads to a lower P(Adelie).
But that was an average effect, now we can use ICE a look at individual instances.

```{r ice-penguins, fig.cap="ICE plot of P(Adelie) by bill length. Each line represents a penguin. We can almost see two types of penguins. Some start with a high probability of being Adelie at a low bill length, and another segment starts with a probability of about 25%."}
library("mlr")
library("ggplot2")
library("palmerpenguins")

set.seed(43)
penguins.task = makeClassifTask(data = penguins, target = "species")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'penguins-rf', predict.type = 'prob'), penguins.task)
pred.penguins = Predictor$new(mod, penguins, class = "Adelie")
ice = FeatureEffect$new(pred.penguins, "bill_length_mm", method = "ice")$plot() + 
  scale_color_discrete(guide='none') + 
  scale_y_continuous('P(Adelie)')
ice
```

The plot shows that the effect of bill length is not the same for each penguin.

The next figure shows ICE plots for the [bike rental prediction](#bike-data).
The underlying prediction model is a random forest.

```{r}
#| label: ice-bike
#| fig-cap: "ICE plots of predicted bicycle rentals by weather conditions. The same effects can be observed as in the partial dependence plots."
#| fig-height: 4

set.seed(42)
data("bike")
bike.subset.index = sample(1:nrow(bike), size = 300)
bike.subset = bike[bike.subset.index,]
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), bike.task)
pred.bike = Predictor$new(mod.bike, bike.subset)

p1 = FeatureEffect$new(pred.bike, "temp", method = "ice")$plot() + scale_x_continuous("Temperature") + 
  scale_y_continuous("Predicted bicycle rentals")
p2 = FeatureEffect$new(pred.bike, "hum", method = "ice")$plot() + scale_x_continuous("Humidity") + scale_y_continuous("")
p3 = FeatureEffect$new(pred.bike, "windspeed", method = "ice")$plot() + scale_x_continuous("Windspeed")+ scale_y_continuous("")
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

All curves seem to follow the same course, so there are no obvious interactions.
That means that the PDP is already a good summary of the relationships between the displayed features and the predicted number of bicycles

#### Centered ICE Plot

There is a problem with ICE plots:
Sometimes it can be hard to tell whether the ICE curves differ between individuals because they start at different predictions.
A simple solution is to center the curves at a certain point in the feature and  display only the difference in the prediction to this point.
The resulting plot is called centered ICE plot (c-ICE).
Anchoring the curves at the lower end of the feature is a good choice.
The new curves are defined as:

$$\hat{f}_{cent}^{(i)}=\hat{f}^{(i)}-\mathbf{1}\hat{f}(x^{a},x^{(i)}_{C})$$

where $\mathbf{1}$ is a vector of 1's with the appropriate number of dimensions (usually one or two), $\hat{f}$ is the fitted model and x^a^ is the anchor point.

#### Example

For example, take the Adelie penguin ICE plot for body mass and center the lines on the penguin with the shortest bill length:

```{r ice-penguins-centered, fig.cap=sprintf("Centered ICE plot for predicted species probabilities by bill length. Lines are fixed to 0 at bill length %.0f. Compared to bill length %.0f, P(Adelie) gets lower for increasing bill length.", min(penguins$bill_length_mm), min(penguins$bill_length_mm)), fig.height = 4}
library("iml")
predictor = Predictor$new(mod, data = penguins, class = "Adelie")
ice = FeatureEffect$new(predictor, feature = "bill_length_mm", center.at = min(penguins$bill_length_mm), method = "pdp+ice")
ice$plot()  +
  scale_color_discrete(guide='none') +
  scale_y_continuous(sprintf('Difference in P(Adelie) to bill length %.0f', min(penguins$bill_length_mm))) 
```

The centered ICE plots make it easier to compare the curves of individual instances.
This can be useful if we do not want to see the absolute change of a predicted value, but the difference in the prediction compared to a fixed point of the feature range.

Let's have a look at centered ICE plots for the bicycle rental prediction:

```{r}
#| label: ice-bike-centered
#| fig-cap: "Centered ICE plots of predicted number of bikes by weather condition. The lines show the difference in prediction compared to the prediction with the respective feature value at its observed minimum."
#| fig-height: 3.3

data(bike)
set.seed(43)
bike.subset.index = sample(1:nrow(bike), size = 100)
bike.subset = bike[bike.subset.index,]

predictor = Predictor$new(mod.bike, data = bike.subset)
ytext1 = sprintf("Different to prediction at temp = %.1f", min(bike$temp))
ice1 = FeatureEffect$new(predictor, feature = "temp", center.at = min(bike$temp), method = "pdp+ice")$plot() +
  scale_y_continuous(ytext1)
ytext2 = sprintf("Different to prediction at hum = %.1f", min(bike$hum))
ice2 = FeatureEffect$new(predictor, feature = "hum", center.at = min(bike$hum), method = "pdp+ice")$plot() +
  scale_y_continuous(ytext2)
ytext3 = sprintf("Different to prediction at windspeed = %.1f", min(bike$windspeed))
ice3 = FeatureEffect$new(predictor, feature = "windspeed", center.at = min(bike$windspeed), method = "pdp+ice")$plot() +
  scale_y_continuous(ytext3)
gridExtra::grid.arrange(ice1, ice2, ice3, nrow = 1)
```

#### Derivative ICE Plot

Another way to make it visually easier to spot heterogeneity is to look at the individual derivatives of the prediction function with respect to a feature.
The resulting plot is called the derivative ICE plot (d-ICE).
The derivatives of a function (or curve) tell you whether changes occur and in which direction they occur.
With the derivative ICE plot, it is easy to spot ranges of feature values where the black box predictions change for (at least some) instances.
If there is no interaction between the analyzed feature $x_S$ and the other features $x_C$, then the prediction function can be expressed as:

$$\hat{f}(x)=\hat{f}(x_S,x_C)=g(x_S)+h(x_C),\quad\text{with}\quad\frac{\delta\hat{f}(x)}{\delta{}x_S}=g'(x_S)$$

Without interactions, the individual partial derivatives should be the same for all instances.
If they differ, it is due to interactions and it becomes visible in the d-ICE plot.
In addition to displaying the individual curves for the derivative of the prediction function with respect to the feature in S, showing the standard deviation of the derivative helps to highlight regions in feature in S with heterogeneity in the estimated derivatives.
The derivative ICE plot takes a long time to compute and is rather impractical.


### Advantages

Individual conditional expectation curves are **even more intuitive to understand** than partial dependence plots.
One line represents the predictions for one instance if we vary the feature of interest. 

Unlike partial dependence plots, ICE curves can **uncover heterogeneous relationships**.

### Disadvantages 

ICE curves **can only display one feature** meaningfully, because two features would require the drawing of several overlaying surfaces and you would not see anything in the plot.

ICE curves suffer from the same problem as PDPs: 
If the feature of interest is correlated with the other features, then **some points in the lines might be invalid data points** according to the joint feature distribution. 

If many ICE curves are drawn, the **plot can become overcrowded** and you will not see anything. 
The solution: Either add some transparency to the lines or draw only a sample of the lines.

In ICE plots it might not be easy to **see the average**. 
This has a simple solution:
Combine individual conditional expectation curves with the partial dependence plot.

### Software and Alternatives

ICE plots are implemented in the R packages `iml` (used for these examples), `ICEbox`[^ICEbox], and `pdp`.
Another R package that does something very similar to ICE is `condvis`. In Python, partial dependence plots are built into scikit-learn starting with version 0.24.0.



[^ICEbox]: Goldstein, Alex, Adam Kapelner, Justin Bleich, and Maintainer Adam Kapelner. "Package ‘ICEbox’." (2017).

[^Goldstein2017]: Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation." journal of Computational and Graphical Statistics 24, no. 1 (2015): 44-65.

