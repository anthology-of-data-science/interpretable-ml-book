# About the Book {-}

## Summary

Machine learning is part of our products, processes, and research.
But **computers usually don't explain their predictions** which can cause many problems, ranging from trust issues to undetected bugs.
This book is about making machine learning models and their decisions interpretable.

After exploring the concepts of interpretability, you will learn about simple, **interpretable models** such as decision trees and linear regression.
The focus of the book is on model-agnostic methods for **interpreting black box models**, ranging from methods for explaining individual predictions, such as SHAP and LIME, to interpreting more general relations between features and predictions with permutation feature importance and accumulated local effects.
In addition, the book presents methods specific to deep neural networks.

All interpretation methods are explained in depth and discussed critically.
How do they work?
What are their strengths and weaknesses?
How to interpret them?
This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.
Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.

##  Why I wrote the book

<!-- high-level view of books journey -->
This book began as a side project while I was working as a statistician in clinical research.
On my free day, I explored topics that interested me, and interpretable machine learning eventually caught my focus.
Expecting plenty of resources on this important subject, I was surprised to find only scattered research papers and blog posts, with no comprehensive guide.
This motivated me to create the resource I wished existed -- a book to both deepen my own understanding and share insights on interpretable machine learning with others.
Today this book is a go-to resource for interpreting machine learning models.
It has been cited by thousands of researchers in their papers, students messaged me that it was essential to their theses, instructors use it in their classes, and data scientists in industry rely on it for their daily work and recommend it to their colleagues.
The book has also been the foundation of my own career; first, it inspired me to do a PhD on interpretable machine learning and later it encouraged me to become self-employed with writing and consulting.
## Who this book is for

This book is for practitioners looking for an overview of techniques to make machine learning models more interpretable.
Itâ€™s also valuable for students, teachers, researchers, and anyone interested in the topic.
A basic understanding of machine learning and basic university-level math will help in following the theory, but the intuitive explanations at the start of each chapter should be accessible without math knowledge.

## What's new in 3rd edition?

Edition three is both a small and a big update.
It's a small update because I only added two new method chapters, namely [LOFO](#lofo) and [Ceteris Paribus](#ceteris-paribus), and two introductory chapters ([Methods Overview](#overview) and [Goals of Interpretability](#goals)).
But I also made some bigger changes to the book that are more subtle, but I believe made the book much better.
I reorganized the introduction part so that it's much leaner, yet more insightful.
Further, I gave the application examples much more depth (e.g., studying correlations and doing more nuanced analysis) and replaced the cancer dataset with the more accessible Palmer penguin dataset.
To make the book more practical, I introduced tips and warning boxes for applying interpretable machine learning techniques the right way.
A huge part was also cleaning up the repository and rendering the book with Quarto instead of bookdown.
For you as the reader this is only visible in the appearance of the web version of the book, but it also means the book is now much easier to maintain for me and this will benefit the Interpretable Machine Learning book in the long run.
I also fixed a lot of small things, which you can see in the [book repository's README](https://github.com/christophM/interpretable-ml-book), section "Changelog".

## About the author

Hi!  
My name is Christoph Molnar I write and teach about machine learning, specifically topics that go beyond merely predictive performance.
I studied statistics, worked for a few years as a data scientist, did my PhD on interpretable machine learning, and now I'm a writer and also [offer workshops](https://christophmolnar.com/workshops/) and [consulting](https://christophmolnar.com/consulting/).
To stay up to date with my work on machine learning, you can subscribe to my newsletter [Mindful Modeler](https://mindfulmodeler.substack.com/).


![](./images/by-nc-sa.png)

This book is licensed under the [CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/) license.

