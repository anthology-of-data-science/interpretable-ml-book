# Introduction {#intro}


"What is 2 + 5?" asked teacher Wilhelm van Osten.
The answer, of course, was 7.
The crowd that had gathered to witness this spectacle was amazed.
Because the answer was given by a horse called "Clever Hans".
Clever Hans could do math -- or so it seemed.
2 + 5?
That's seven taps with the horse's foot and not one more.
Quite impressive for a horse.

And indeed, Clever Hans was very clever, as later investigations showed.
But his skills were not in math, but in reading social cues.
It turned out that an important success factor was that the human asking Hans knew the answer.
And Hans relied on the tiniest changes in the human's body language and facial expressions to stop tapping at the right time.

## You can't trust models based on performance alone

In machine learning, we also have our versions of this clever horse: Clever Hans Predictors (term coined by @lapuschkinUnmaskingCleverHans2019).
For example:

- A machine learning model trained to detect whales learned to rely on artifacts in audio files instead of basing the classification on the audio content [@kaggle2013challenge].
- An image classifier learned to use text on images instead of visual features [@lapuschkinUnmaskingCleverHans2019].
- A wolf versus dog classifier relied on snow features instead of using features of the animals [@ribeiro2016why].

<!-- need for interpretability -->
And for all these examples, the flaws were not visible in the evaluation metric.
So it's not surprising that, even though machine learning had great success, people are wary, even for well-performing models.
They want a way to look inside the models, to make sure they are not taking short cuts. 
And there are many other reasons to look into the model.
More and more researchers are using machine learning in their work.
In a survey asking scientists for their biggest concerns about using machine learning, the top concern was “Leads to more reliance on pattern recognition without understanding” [@vannoordenAIScienceWhat2023].
This lack of understanding is not unique to science.
If you work in marketing and are building a churn model, you want to predict not only who is likely to churn, but also why.
Otherwise, how would the marketing team know what the right response is?
The team could send everyone a voucher, but what if the reason for high churn probability was that they are annoyed by the many emails?
Good predictive performance wouldn't be enough to make full use of the churn model.
Many data scientists and statisticians have told me that one reason they are using "simpler models" is that they couldn't convince their boss to use a "black box model" or there are regulatory reasons.

<!-- solution: interpretability -->
To solve trust issues, allowing insights into the models and too better debug the models, you are reading the right book.
Interpretable Machine Learning offers the tools to extract insights from the model. 
Interpretability helps developers with debugging and improvements, builds trust in the model, justifies model predictions and leads to new insights.
The increased need for machine learning interpretability is a natural consequence of an increased use of machine learning.


## Interpretable ML is a young field with old roots

<!-- History of IML -->
Linear regression models were already used at the beginning of the 19th century. 
[@legendre1806nouvelles,@gauss1877theoria].
Statistical modeling grew around that linear regression model, and today we have more options like generalized additive models to LASSO to name some popular ones.
Classic statistical modeling centers around the idea of modeling the distribution of data relying on assumptions and using that model then to make conclusions about the world.
To do that, interpretability is key.
For example, if you model the effect of drinking alcohol on risk for cardio-vascular problems, statisticians need to be able to extract that insight from the model.
This is typically done by keeping the model interpretable and having a coefficient that can be interpreted as effect of a feature on the outcome. 

Machine learning has a different modeling approach.
More complex models, more focus on algorithms than distributions, more focused on prediction, and more task-driven.
Most machine learning research started in the later half of the 20th century.
Neural networks go back to the 1960s [@schmidhuber2015deep].
Rule-based machine learning, which can be counted to the interpretable ML, are an active research area since the mid of the 20th century.
And also for many of these complex models, interpretability was a concern: 
An example would be the random forest [@breiman2001random] which already came with built-in feature importance measure.

But Interpretable ML or Explainable AI has really exploded as a field around 2015 [@molnar2020interpretable], and especially the subfield of model-agnostic interpretability, which offers method that work for any model.
New methods for the interpretation of machine learning models are still being published at breakneck speed.
To keep up with everything that is published would be madness and simply impossible.
That is why you will not find the most novel and fancy methods in this book, but established methods and basic concepts of machine learning interpretability.
These basics prepare you for making machine learning models interpretable.
Internalizing the basic concepts also empowers you to better understand and evaluate any new paper on interpretability published on [arxiv.org](https://arxiv.org/) in the last 5 minutes since you began reading this book (I might be exaggerating the publication rate).


## How to read the book

You don't have to read the book cover from cover, since Interpretable Machine Learning has a reference book character with most chapters describing one method.
If you are new to interpretability, I only would recommend reading the [Interpretability Chapter](#interpretability) and the [Methods Overview Chapter](#overview) first to understand what interpretability is all about and to have a "map" where you can place each method. 

The book is organized into the following parts: 

- The introductory chapters, including interpretability definitions and methods overview 
- Interpretable models 
- Local model-agnostic methods
- Global model-agnostic methods
- Interpretation methods for neural networks
- Outlook part, including short stories, speculations about the future, and related fields.
- Appendices for terminology and explaining the datasets

Each method chapter follows a similar structure:
The first paragraph summarizes the method, followed by an intuitive explanation that doesn't rely on math.
Then we look into the theory of the method to get a deeper understanding of how it works, including math and algorithms.
I believe that a new method is best understood using examples.
Therefore, each method is applied to real data.
Some people say that statisticians are very critical people.
For me, this is true, because each chapter contains critical discussions about pros and cons of the respective interpretation method.
This book is not an advertisement for the methods, but it should help you decide whether a method is a good fit for your project or not.
In the last section of each chapter, available software implementations are discussed.

I hope you will enjoy the read!
